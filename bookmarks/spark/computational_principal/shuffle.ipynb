{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "font-family\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "font-family\n",
    "\n",
    "\"objektiv-mk1\",Helvetica,Arial,sans-serif !important\n",
    "\n",
    "\"Helvetica Neue\", Helvetica, Roboto, Arial, sans-serif\n",
    "\n",
    "Linotte Light\n",
    "\n",
    "Clear Sans\n",
    "\"\"\"\n",
    "\n",
    "print('font-family')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# google: what is shuffle in spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [The main article to reference: Apache Spark - Performance](https://blog.scottlogic.com/2018/03/22/apache-spark-performance.html)\n",
    "\n",
    "The task today is to process the London Cycle Hire data into <font color='red'>two separate sets, Weekends and Weekdays</font>. \n",
    "\n",
    "<font color='red'>Grouping data into smaller subsets for further processing</font> is a common business requirement\n",
    "\n",
    "and we’re going to see how Spark can help us with the task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Partitioning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ideal Partitioning\n",
    "\n",
    "<p style=\"text-align: center; font-family: 'objektiv-mk1,Helvetica,Arial,sans-serif,Linotte Light';\">\n",
    "    To <font color='#52bd86'>distribute work across the cluster</font> and <font color='#52bd86'>reduce the memory requirements of each node</font>, \n",
    "    Spark will <font color='red' size='5px'>split the data into smaller parts</font> called <font color='red' size='5px'>Partitions</font>.\n",
    "</p>\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src='https://blog.scottlogic.com/mdebeneducci/assets/Ideal-Partitioning.png' width='500'/>\n",
    "            <p style=\"text-align: center; font-family: 'objektiv-mk1';\"> Fig: Diagram of Ideal Partitioning </p>\n",
    "        </td>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imperfect Partitioning - when data is split into partitions\n",
    "\n",
    "#### Data Skew\n",
    "\n",
    "<font color='#0091fa'>Often the data is split into partitions based on a key</font>, for instance the first letter of a name. \n",
    "If values are <font color='red'>not evenly distributed throughout this key</font> then more data will be placed in one partition than another. An example would be:\n",
    "\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td style=\"white-space:pre-wrap; word-wrap:break-word\">\n",
    "            {Adam, Alex, Anja, Beth, Claire}\n",
    "            -> A: {Adam, Alex, Anja}\n",
    "            -> B: {Beth}\n",
    "            -> C: {Clair}\n",
    "        </td>\n",
    "        <td>\n",
    "            <img src='https://blog.scottlogic.com/mdebeneducci/assets/Skewed-Partitions.png' width='500'/>\n",
    "            <p style=\"text-align: center;\"> Fig: Diagram of Skewed Partitioning </p>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n",
    "\n",
    "\n",
    "- Here the A partition is 3 times larger than the other two, and therefore will take approximately 3 times as long to compute. <font color='#0091fa' size='3px'>As the next stage of processing cannot begin until all three partitions are evaluated, the overall results from the stage will be delayed.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scheduling\n",
    "\n",
    "- If <font color='red' size='3px'>too few partitions to correctly cover the number of executors available</font>, this results in <font color='#0091fa'>Executor</font> 2 being <font color='#0091fa'>idle and unused for half the job time</font>.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src='https://blog.scottlogic.com/mdebeneducci/assets/Inefficient-Scheduling.png' width='500'/>\n",
    "            <p style=\"text-align: center;\"> Fig: Diagram of Badly Scheduled Partitioning </p>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution\n",
    "\n",
    "The simplest solution to the above two problems is to <font color='#52bd86'>increase the number of partitions</font> used for computations. This will reduce the effect of skew into a single partition and will also allow better matching of scheduling to CPUs.\n",
    "\n",
    "<font color='#0091fa' size='3px'>A common recommendation is to have 4 partitions per CPU</font>, <font color='red' size='5px'>however settings related to Spark performance are very case dependent, and so this value should be fine-tuned with your given scenario.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shuffling\n",
    "\n",
    "\n",
    "A <font color='red' size='5px'>shuffle</font> occurs when <font color='red' size='5px'>data is rearranged between partitions.</font>\n",
    "This is required when a transformation requires information from other partitions, such as summing all the values in a column. <font color='#0091fa'>Spark will gather the required data from each partition and combine it into a new partition, likely on a different executor.</font>\n",
    "\n",
    "\n",
    "<font color='red' size='5px'>During a shuffle, data is written to disk and transferred across the network, halting Spark’s ability to do processing in-memory and causing a performance bottleneck.</font> Consequently we want to try to reduce the number of shuffles being done or reduce the amount of data being shuffled.\n",
    "\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src='https://blog.scottlogic.com/mdebeneducci/assets/Shuffle-Diagram.png' width='500'/>\n",
    "            <p style=\"text-align: center;\"> Fig: Diagram of Shuffling Between Executors </p>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Map-Side Reduction\n",
    "\n",
    "\n",
    "When aggregating data during a shuffle, rather than pass all the data, it is <font color='#52bd86' size='3px'>preferred to combine the values in the current partition and pass only the result in the shuffle</font>. This process is known as <font color='#52bd86' size='3px'>**Map-Side Reduction**</font> and improves performance by reducing the quantity of data being transferred during a shuffle.\n",
    "\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src='https://blog.scottlogic.com/mdebeneducci/assets/Map-Side-Reduction.png' width='500'/>\n",
    "            <p style=\"text-align: center;\"> Fig: Diagram of Map-Side Reduction </p>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In Practice\n",
    "\n",
    "\n",
    "### Round 1\n",
    "\n",
    "<font color='#52bd86' size='3px'>When the job is run, we see the repartition command does a shuffle and produces 200 partitions (the spark default)</font>, which should offer excellent levels of parallelisation; let’s look at the execution timeline.\n",
    "\n",
    "- <font color='#ffb804'>The timeline does not look balanced.</font>\n",
    "\n",
    "In this case it has occurred because <font color='#4cf0c2' size='4px'>calling `repartition` moves all values for the same key into the same partition on one Executor.</font> <font color='red' size='3px'>Here our key isWeekend is a boolean value, meaning that only two partitions will be populated with data. Spark is not able to account for this in its internal optimisation and therefore offers 198 other partitions with no data in them.</font> If we had more than two executors available, they would receive only empty partitions and would be idle throughout this process, greatly reducing the total throughput of the cluster.\n",
    "\n",
    "\n",
    "Grouping in this fashion is also a common source of memory exceptions as, with a large data set, a single partition can easily be given multiple GBs of data and quickly exceed the allocated RAM. Therefore we must consider the likely proportion of data for each key we have chosen and how that correlates to our cluster.\n",
    "\n",
    "\n",
    "---\n",
    "data<font color='#40e0d0' size='5px'>.repartition(</font>data.col(\"isWeekend\")<font color='#40e0d0' size='5px'>).write()</font>\n",
    "        .parquet(\"cycle-data-results\" + Time.now());\n",
    "---\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src='https://blog.scottlogic.com/mdebeneducci/assets/Unbalanced-Shuffles.png' width='800'/>\n",
    "            <p style=\"text-align: center;\"> Fig: 200 Partitions Execution Timeline and Metrics </p>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Round 2\n",
    "\n",
    "To improve on the above issues, we need to make changes to our query so that it more evenly spreads the data across our partitions and executors.\n",
    "\n",
    "<font color='red'>Another way of writing the 'repartition query' is to <font color='red' size='5px'>delegate the repartitioning to the write method</font>.</font>\n",
    "\n",
    "### reason\n",
    "\n",
    "- In the previous case Spark loaded the CSV files into 69 partitions, split these based on isWeekend and shuffled the results into 200 new partitions for writing. \n",
    "\n",
    "- In the new solution Spark still loads the CSVs into 69 partitions, however it is then able to <font color='red' size='5px'>skip the shuffle stage, realising that it can split the existing partitions based on the key and then write that data directly to parquet files</font>. Looking at the execution timeline, we can see a much healthier spread between the partitions and the nodes, and no shuffle occurring.\n",
    "\n",
    "---\n",
    "data<font color='red' size='5px'>.write().partitionBy(</font>\"isWeekend\"<font color='red' size='5px'>)</font>\n",
    "        .parquet(\"cycle-data-results\" + Time.now());\n",
    "---\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <td>\n",
    "            <img src='https://blog.scottlogic.com/mdebeneducci/assets/Better-Balancing.png' width='800'/>\n",
    "            <p style=\"text-align: center;\"> Fig: 200 Partitions Execution Timeline and Metrics </p>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion\n",
    "\n",
    "> A repository with the example code can be found [here](https://github.com/MatdeB-SL/Spark-Performance---Cycle-Hire-Data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [repartition() vs partitionBy()](https://sparkbyexamples.com/pyspark/pyspark-repartition-vs-partitionby/)\n",
    "\n",
    "- `repartition()` creates a specified number of partitions in memory\n",
    "\n",
    "- `partitionBy()` will write files to disk for each memory partition and partition column. \n",
    "\n",
    "---\n",
    "\n",
    "### [Difference between df.repartition and DataFrameWriter partitionBy?](https://stackoverflow.com/questions/40416357/difference-between-df-repartition-and-dataframewriter-partitionby)\n",
    "\n",
    "--\n",
    "\n",
    "\n",
    "The first part of the accepted answer is correct: calling `df`<font color='#40e0d0' size='5px'>.repartition(</font>COL, numPartitions=k<font color='#40e0d0' size='5px'>)</font> will <font color='#40e0d0' size='5px'>create a dataframe with k partitions using a hash-based partitioner</font>. <font color='#ffb804'>COL</font> here <font color='#ffb804'>defines the partitioning key</font>--it can be a single column or a list of columns. <font color='#40e0d0' size='3px'>The hash-based partitioner takes each input row's partition key, hashes it into a space of k partitions</font> via something like `partition = hash(partitionKey) % k`. This guarantees that all rows with the same partition key end up in the same partition. However, rows from multiple partition keys can also end up in the same partition (when a hash collision between the partition keys occurs) and some partitions might be empty.\n",
    "\n",
    "In summary, the unintuitive aspects of `df.repartition(COL, numPartitions=k)` are that\n",
    "\n",
    "- partitions will not strictly segregate partition keys\n",
    "\n",
    "- some of your <font color='#40e0d0' size='5px'>k</font> partitions may be empty, whereas others may contain rows from multiple partition keys\n",
    "\n",
    "\n",
    "--\n",
    "\n",
    "\n",
    "The behavior of `df`<font color='red' size='5px'>.write.partitionBy</font> is quite different, in a way that many users won't expect. Let's say that you want your output files to be date-partitioned, and your data spans over 7 days. Let's also assume that df has 10 partitions to begin with. When you run df.write.partitionBy('day'), how many output files should you expect? The answer is 'it depends'. If each partition of your starting partitions in df contains data from each day, then the answer is 70. If each of your starting partitions in df contains data from exactly one day, then the answer is 10.\n",
    "\n",
    "How can we explain this behavior? When you run <font color='red' size='5px'>df.write, each of the original partitions in df is written independently</font>. That is, each of your original 10 partitions is sub-partitioned separately on the 'day' column, and a separate file is written for each sub-partition.\n",
    "\n",
    "I find this behavior rather annoying and wish there were a way to do a global repartitioning when writing dataframes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession  # , Window\n",
    "# from pyspark.sql.types import *\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "\n",
    "spark_mode = 'local'\n",
    "\n",
    "\n",
    "def spark_session(mode='yarn', machine='hadoop_node'):\n",
    "    \"\"\"\n",
    "    machine=['hadoop_node', 'mac']\n",
    "    mode=['yarn', 'local']\n",
    "    \"\"\"\n",
    "    APP_NAME = 'get_history_vbs_by_batt'\n",
    "    if machine == 'hadoop_node':\n",
    "        spark_sess = SparkSession \\\n",
    "            .builder \\\n",
    "            .appName(APP_NAME) \\\n",
    "            .master(mode) \\\n",
    "            .getOrCreate()\n",
    "    else:\n",
    "        # add config to bind host and address to your machine ip when testing on Mac locally\n",
    "        spark_sess = SparkSession \\\n",
    "            .builder \\\n",
    "            .appName(APP_NAME) \\\n",
    "            .master(mode) \\\n",
    "            .config('spark.driver.host', '127.0.0.1') \\\n",
    "            .config('spark.driver.bindAddress', '127.0.0.1') \\\n",
    "            .getOrCreate()\n",
    "    return spark_sess\n",
    "\n",
    "\n",
    "spark = spark_session(mode=spark_mode, machine='hadoop_node')\n",
    "spark.sparkContext.setLogLevel('WARN')  # to reduce unnecessary logs\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- [code below - reference](https://intellipaat.com/community/16097/pyspark-repartition-vs-partitionby)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pairs.partitionBy(3).glom().collect(): [[(3, 3), (6, 6), (6, 6)], [(1, 1), (4, 4), (4, 4), (1, 1), (7, 7), (7, 7), (4, 4)], [(2, 2), (2, 2), (5, 5), (5, 5), (5, 5)]]\n",
      "pairs_2.repartition(3).glom().collect(): [[], [(1, 1), (2, 2), (3, 3), (4, 4), (2, 2), (4, 4), (1, 1), (5, 5), (6, 6), (7, 7)], [(7, 7), (5, 5), (5, 5), (6, 6), (4, 4)]]\n"
     ]
    }
   ],
   "source": [
    "pairs = sc.parallelize([1, 2, 3, 4, 2, 4, 1, 5, 6, 7, 7, 5, 5, 6, 4]).map(lambda x: (x, x))\n",
    "\n",
    "print('pairs.partitionBy(3).glom().collect():', pairs.partitionBy(3).glom().collect())\n",
    "\n",
    "# [[(3, 3), (6, 6), (6, 6)],\n",
    "#  [(1, 1), (4, 4), (4, 4), (1, 1), (7, 7), (7, 7), (4, 4)],\n",
    "#  [(2, 2), (2, 2), (5, 5), (5, 5), (5, 5)]]\n",
    "\n",
    "pairs_2 = sc.parallelize([1, 2, 3, 4, 2, 4, 1, 5, 6, 7, 7, 5, 5, 6, 4]).map(lambda x: (x, x))\n",
    "print('pairs_2.repartition(3).glom().collect():', pairs_2.repartition(3).glom().collect())\n",
    "\n",
    "# [[(4, 4), (2, 2), (6, 6), (7, 7), (5, 5), (5, 5)],\n",
    "#  [(1, 1), (4, 4), (6, 6), (4, 4)],\n",
    "#  [(2, 2), (3, 3), (1, 1), (5, 5), (7, 7)]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Spark Partitions](https://luminousmen.com/post/spark-partitions)\n",
    "\n",
    "Let us imagine that the size of our input dataset is about 30 GB (~30000 MB) of an uncompressed text file on the HDFS which is distributing it on 10 nodes.\n",
    "\n",
    "When Spark reads a file from HDFS, it creates a single partition for a single input split. Input split is set by the Hadoop InputFormat used to read this file. If you have a 30GB uncompressed text file stored on HDFS, then with the default HDFS block size setting (128MB) and default spark.files.maxPartitionBytes(128MB) it would be stored in 240 blocks, which means that the dataframe you read from this file would have 240 partitions.\n",
    "\n",
    "This is equal to the Spark default parallelism (spark.default.parallelism) value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# spark 3\n",
    "https://teepika-r-m.medium.com/apache-spark-3-0-exciting-capabilities-f57132e158f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# different spark mode\n",
    "\n",
    "cluster mode, client mode, local mode \n",
    "-> figure illustration\n",
    "https://blog.knoldus.com/cluster-vs-client-execution-modes-for-a-spark-application/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
